// TODO: elaborate further on many sections   
// TODO: need a lot more examples. 



# CLR memory model

## ECMA vs. CLR memory models.
ECMA 335 standard defines a very weak memory model. After two decades the desire to have a flexible model did not result in considerable benefits due to hardware being stricter. On the other hand programming against ECMA model requires extra complexity to handle scenarios that are hard to comprehend and not possible to test.

In the course of multiple releases CLR implementation settled around a memory model that is a practical compromise between what can be implemented efficiently on the current hardware, while staying reasonably approachable by the developers.  
This document rationalizes the invariants provided and expected by the CLR runtime in its current implementation with expectation of that being carried to future releases.

While the memory model is generally the same among runtime implementations such as .Net FX, CoreCLR, Mono, NativeAOT, when discrepancies do happen they will be called out.

## Hardware considerations
Current CoreCLR and libraries implementation makes a few expectations about the hardware memory model. These conditions are present on all currently supported platforms and transparently passed to the user of the runtime. The future supported platforms will likely support these too as the large body of preexisting software will make it burdensome to break common assumptions. 

* Naturally aligned reads and writes with sizes up to the platform pointer size are atomic.   
That applies even for locations targeted by overlapping aligned reads and writes of different sizes.  
**Example:** a read of a 4-byte aligned int32 variable will yield a value that existed prior some write or after some write. It will never be a mix of before/after bytes.

*	The memory is cache-coherent and writes to a single location will be seen by all cores in the same order (multicopy atomic).  
**Example:** when the same location is updated with values in ascending order (like 1,2,3,4,...), no observer will see a descending sequence.

*	It may be possible for a thread to see its own writes before they appear to other cores (store buffer forwarding), as long as the single-thread consistency is not violated.

*	The memory managed by the runtime is ordinary memory (not device register file or the like) and the only sideeffects of memory operations are storing and reading of values.

*	It is possible to implement release consistency memory model.  
Either the platform defaults to release consistency or stronger (i.e. x64 is TSO, which is stronger), or provides means to implement release consistency via fencing operations.

*	Memory ordering honors data dependency  
**Example:** reading a field, will not use a cached value fetched from the location of the field prior obtaining a reference to the instance.  
(Some versions of Alpha processors did not support this, most current architectures do)

## Alignment
Data managed by CLR is *properly aligned* according to the data type size.

1,2,4 - byte data types are aligned on 1,2,4 byte granularity. This applies to both heap and stack allocated memory.

8-byte data types are 8-byte aligned on 64 bit platforms.

Native-pointer-sized datatypes are always aligned.

## Atomic memory accesses.
Memory accesses to *properly aligned* data are always atomic. The value that is observed is always a result of complete read and write operations.

## Unmanaged memory access. 
As pointers can point to any addressable memory, operations with pointers may violate guarantees provided by the runtime and expose undefined or platform-specific behavior.  
**Example:** memory accesses through pointers which are *not properly aligned* may be not atomic or cause faults depending on the platform and hardware configuration.   

Although rare, unaligned access is a realistic scenario and thus there is some limited support for unaligned memory accesses, such as `.unaligned` IL prefix and `Unsafe.ReadUnaligned`, `Unsafe.WriteUnaligned` and ` Unsafe.CopyBlockUnaligned` helpers. 
These facilities ensure fault-free access, but do not ensure atomicity.

As of this writing there is no specific support for operating with incoherent memory, device memory or similar. Support for unusual kinds of memory may be added if/when needed.

## Sideeffects and optimizations of memory accesses.
CLR assumes that the sideeffects of memory reads and writes include only changing and observing values at specified memory locations. This applies to all reads and writes - volatile or not. **This is different from ECMA model.**

As a consequence: 
* Speculative writes are not allowed. 
*	Reads cannot be introduced.
*	Unused reads can be elided.
*	Adjacent reads from the same location can be coalesced.
*	Adjacent writes to the same location can be coalesced.

## Thread-local memory accesses.
It may be possible for an optimizing compiler to prove that some data is accessible only by a single thread. In such case it is permitted to perform further optimizations such as duplicating or removal of memory accesses.  

## Order of memory operations.
* **Ordinary memory accesses**  
The effects of ordinary reads and writes can be reordered as long as that preserves single-thread consistency. Such reordering can happen both due to code generation strategy of the compiler or due to weak memory ordering in the hardware. 
Volatile memory accesses

*	**Volatile reads** have "acquire semantics" - no read or write that is later in the program order may be speculatively executed ahead of a volatile read.  
  Operations with acquire semantics:  
     - IL load instructions with `.volatile` prefix when such prefix is supported
     - Volatile.Read

*	**Volatile writes** have "release semantics" - the effects of a volatile write will not be observable before effects of all previous, in program order, reads and writes become observable.  
  Operations with release semantics:
     - IL store instructions with `.volatile` prefix when such prefix is supported
     - Volatile.Write
     - Releasing a lock

Note that volatile semantics does not imply that operation is atomic or has any effect on how soon the operation is committed to the coherent memory. It only specifies the order of effects when they eventually become observable.

* **Full-fence operations**  
  Full-fence operations have "full-fence semantics" - effects of reads and writes must be observable no later or no earlier than a full-fence operation according to their relative program order.  
  Operations with full-fence semantics:
     - Thread.MemoryBarrier
     - Interlocked methods
     - Lock acquire

For historical reasons:  
     - Thread.VolatileRead performs a full fence after the read  
     - Thread.VolatileWrite performs a full fence prior to the write

## Process-wide barrier
Process-wide barrier has full-fence semantics with an additional guarantee that each thread in the program effectively performs a full fence at arbitrary point synchronized with the process-wide barrier in such a way that effects of writes that precede both barriers are observable by memory operations that follow the barriers.  

The actual implementation may vary depending on the platform. For example interrupting the execution of every core in the current process' affinity mask could be a suitable implementation.

## Object assignment
Object assignment to a location potentially accessible by other threads is a release with respect to write operations to the instanceâ€™s fields and metadata.
The motivation is to ensure that storing an object reference to shared memory acts as a "committing point" to all modifications that are reachable through the instance reference. It also guarantees that a freshly allocated instance is valid (i.e. method table and necessary flags are set) when other threads, including background GC threads are able to access the instance.  
The reading thread does not need to perform an acquiring read before accessing the content of an instance since all supported platforms honor ordering of data-dependent reads. 

However, the ordering sideeffects of reference assignement should not be used for general ordering purposes because: 
-	ordinary reference assignments are still treated as ordinary assignments and could be reordered by the compiler. 
-	an optimizing compiler can omit the release semantics if it can prove that the instance is not shared with other threads.  

## Instance constructors
CLR does not prescribe any ordering effects to instance constructors.

## Static constructors
All side effects of static constructor execution must happen before accessing any member of the type.

//TODO: is this a case when RunClassConstructor is called?

## Referential transparency.
Implementation of refs, pointers, remoting, MarshalByRefObject

## Exceptions
Synchronous, asynchronous, thread.abort. 
Anything to do with memory model?
