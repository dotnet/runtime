########################################################################################################
#
# Finalize SuperPMI collection: (1) merge all MCH files generated by all Helix jobs, (2) upload MCH file
# to Azure Storage, (3) upload log files. Note that the steps are "condition: always()" because we want
# to upload as much of the collection as possible, even if there were test failures.
#
########################################################################################################

parameters:
  buildConfig: ''
  buildConfigUpper: ''
  osGroup: ''
  osSubgroup: ''
  archType: ''
  SuperPmiCollectionType: 'run'
  SuperPmiCollectionName: 'libraries_tests'
  MergedMchFileLocation: ''
  MchFilesLocation: ''
  SpmiLogsLocation: ''
  SuperPmiMcsPath: ''
  PythonScript: ''
  PipScript: ''

steps:

  # Create required directories for merged mch collection and superpmi logs
  - ${{ if ne(parameters.osGroup, 'windows') }}:
    - script: |
        mkdir -p ${{ parameters.MergedMchFileLocation }}
        mkdir -p ${{ parameters.SpmiLogsLocation }}
      displayName: 'Create SuperPMI directories'
      condition: always()
  - ${{ if eq(parameters.osGroup, 'windows') }}:
    - script: |
        mkdir ${{ parameters.MergedMchFileLocation }}
        mkdir ${{ parameters.SpmiLogsLocation }}
      displayName: 'Create SuperPMI directories'
      condition: always()

  - script: ${{ parameters.PythonScript }} $(Build.SourcesDirectory)/src/coreclr/scripts/superpmi.py merge-mch -log_level DEBUG -pattern ${{ parameters.MchFilesLocation }}${{ parameters.SuperPmiCollectionName }}.${{ parameters.SuperPmiCollectionType }}*.mch -output_mch_path ${{ parameters.MergedMchFileLocation }}${{ parameters.SuperPmiCollectionName }}.${{ parameters.SuperPmiCollectionType }}.${{ parameters.osGroup }}.${{ parameters.archType }}.${{ parameters.buildConfig }}.mch -core_root ${{ parameters.SuperPmiMcsPath }}
    displayName: 'Merge ${{ parameters.SuperPmiCollectionName }}-${{ parameters.SuperPmiCollectionType }} SuperPMI collections'
    condition: always()

  - template: /eng/pipelines/common/upload-artifact-step.yml
    parameters:
      rootFolder: ${{ parameters.MergedMchFileLocation }}
      includeRootFolder: false
      archiveType: $(archiveType)
      tarCompression: $(tarCompression)
      archiveExtension: $(archiveExtension)
      artifactName: 'SuperPMI_Collection_${{ parameters.SuperPmiCollectionName }}_${{ parameters.SuperPmiCollectionType }}_${{ parameters.osGroup }}${{ parameters.osSubgroup }}_${{ parameters.archType }}_${{ parameters.buildConfig }}'
      displayName: 'Upload artifacts SuperPMI ${{ parameters.SuperPmiCollectionName }}-${{ parameters.SuperPmiCollectionType }} collection'
      condition: always()

  # Add authenticated pip feed
  - task: PipAuthenticate@1
    displayName: 'Pip Authenticate'
    inputs:
      artifactFeeds: public/dotnet-public-pypi
      onlyAddExtraIndex: false
    condition: always()

  # Ensure the Python azure-storage-blob package is installed before doing the upload.
  - script: ${{ parameters.PipScript }} install --user --upgrade pip && ${{ parameters.PipScript }} install --user azure.storage.blob==12.5.0 --force-reinstall
    displayName: Upgrade Pip to latest and install azure-storage-blob Python package
    condition: always()

  - script: ${{ parameters.PythonScript }} $(Build.SourcesDirectory)/src/coreclr/scripts/superpmi.py upload -log_level DEBUG -arch ${{ parameters.archType }} -build_type ${{ parameters.buildConfig }} -mch_files ${{ parameters.MergedMchFileLocation }}${{ parameters.SuperPmiCollectionName }}.${{ parameters.SuperPmiCollectionType }}.${{ parameters.osGroup }}.${{ parameters.archType }}.${{ parameters.buildConfig }}.mch -core_root $(Build.SourcesDirectory)/artifacts/bin/coreclr/${{ parameters.osGroup }}.x64.${{ parameters.buildConfigUpper }}
    displayName: 'Upload SuperPMI ${{ parameters.SuperPmiCollectionName }}-${{ parameters.SuperPmiCollectionType }} collection to Azure Storage'
    condition: always()
    env:
      CLRJIT_AZ_KEY: $(clrjit_key1) # secret key stored as variable in pipeline

  - task: CopyFiles@2
    displayName: Copying superpmi.log of all partitions
    inputs:
      sourceFolder: '${{ parameters.MchFilesLocation }}'
      contents: '**/${{ parameters.SuperPmiCollectionName }}.${{ parameters.SuperPmiCollectionType }}*.log'
      targetFolder: '${{ parameters.SpmiLogsLocation }}'
    condition: always()

  - task: PublishPipelineArtifact@1
    displayName: Publish SuperPMI logs
    inputs:
      targetPath: ${{ parameters.SpmiLogsLocation }}
      artifactName: 'SuperPMI_Logs_${{ parameters.SuperPmiCollectionName }}_${{ parameters.SuperPmiCollectionType }}_${{ parameters.osGroup }}${{ parameters.osSubgroup }}_${{ parameters.archType }}_${{ parameters.buildConfig }}'
    condition: always()